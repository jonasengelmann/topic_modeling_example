{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic_Modelling_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ExCjpiIoKacl",
        "c5vj-VJZWLen"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUTPgdT-x587",
        "colab_type": "text"
      },
      "source": [
        "#1 Preperations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRg19OPGRvxw",
        "colab_type": "text"
      },
      "source": [
        "Install third party libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_P77IK4tKtT",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "!pip install aiohttp==3.6.2\n",
        "!pip install nest_asyncio==1.4.0\n",
        "!pip install aiosqlite==0.15.0\n",
        "!pip install gensim==3.8.3\n",
        "!pip install pyLDAvis==2.1.2\n",
        "!pip install colorspacious==1.1.2\n",
        "!pip install spacy==2.2.4\n",
        "!pip install nltk==3.4\n",
        "!pip install tqdm==4.41.1\n",
        "!pip install bokeh==2.1.1\n",
        "!pip install beautifulsoup4==4.7.1\n",
        "!pip install pandas==1.0.5\n",
        "!pip install numpy==1.18.5\n",
        "!pip install wordcloud==1.5.0\n",
        "!pip install sklearn==0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ58phyXBzX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import logging\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import date, timedelta\n",
        "from pprint import pprint\n",
        "from collections import defaultdict\n",
        "\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import aiosqlite\n",
        "import sqlite3\n",
        "import tqdm\n",
        "import traceback\n",
        "import nest_asyncio\n",
        "import gensim\n",
        "import spacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from wordcloud import WordCloud\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9W87o5ox3U4",
        "colab_type": "text"
      },
      "source": [
        "Prepare folder structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y75aALetLnsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = Path('01-data')\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "models_dir = Path('02-models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "plots_dir = Path('03-plots')\n",
        "plots_dir.mkdir(exist_ok=True)\n",
        "extra_libraries_dir = Path('04_extra_libraries')\n",
        "os.environ[\"EXTRA_LIBRARIES\"] = str(extra_libraries_dir)\n",
        "extra_libraries_dir.mkdir(exist_ok=True)\n",
        "\n",
        "article_db = data_dir / 'articles.db'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExCjpiIoKacl",
        "colab_type": "text"
      },
      "source": [
        "#2 Building the text corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU8fyxvqp-qy",
        "colab_type": "text"
      },
      "source": [
        "To build a large text corpus, we will scrape some newspaper articles from zeit.de which are publicly available and easily to parse. Newspaper articles in general are a good start to get familiar with topic modelling since we can more or less anticipate distinct topics. \n",
        "\n",
        "To scrape, we will first navigate through zeit.de's sitemap and download all available urls to online articles for a given period. Then, we will download them, parse their content and save the resulting text in a text file, one article per line. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCvh7K6rtLRr",
        "colab_type": "text"
      },
      "source": [
        "We will be using Python's asyncio library which will allows us to write asynchronous code. This way, Python can request multiple articles from the server at once without having to wait for the first request to be completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKnmphkbvSjk",
        "colab_type": "text"
      },
      "source": [
        "We will need this, so asyncio works within Ipython notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMsFsFyWDbbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W0W1CMFUD6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "db.execute(\"\"\"\n",
        "  CREATE TABLE articles\n",
        "    (\n",
        "      article_id INTEGER PRIMARY KEY,\n",
        "      url TEXT UNIQUE,\n",
        "      title TEXT,\n",
        "      text TEXT,\n",
        "      authors TEXT,\n",
        "      publishing_date TEXT,\n",
        "      topic_ref TEXT,\n",
        "      tags TEXT,\n",
        "      downloaded TEXT\n",
        "    )\n",
        "\"\"\")\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-a11zwURFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "async def fetch_html(url, session):\n",
        "      response = await session.request(method=\"GET\", url=url)\n",
        "      response.raise_for_status()\n",
        "      content = await response.text()\n",
        "      return content\n",
        "\n",
        "\n",
        "async def parse_html(url, session):\n",
        "    try:\n",
        "        html = await fetch_html(url=url, session=session)\n",
        "    except (\n",
        "        aiohttp.ClientError,\n",
        "        aiohttp.http_exceptions.HttpProcessingError,\n",
        "    ) as ex:\n",
        "        print(ex)\n",
        "        return None\n",
        "    else:\n",
        "        soup = BeautifulSoup(html, features=\"lxml\")\n",
        "        urls = [entry.loc.text for entry in soup.findAll(\"url\")]\n",
        "        return urls\n",
        "\n",
        "\n",
        "async def write_urls_to_db(db_session, url, session):\n",
        "    result = await parse_html(url=url, session=session)\n",
        "    if result:\n",
        "        for article_url in result:\n",
        "            data = (f\"{article_url}\", \"false\",)\n",
        "            try:\n",
        "                await db_session.execute(\n",
        "                    \"INSERT OR IGNORE INTO articles (url, downloaded) VALUES (?,?)\",\n",
        "                    data\n",
        "                )\n",
        "            except sqlite3.InterfaceError as ex:\n",
        "                print(ex)\n",
        " \n",
        "async def bulk_crawl_and_write(article_db, start_date, end_date):\n",
        "    def daterange(start_date, end_date):\n",
        "        '''Helper function to easily iterate over date range'''\n",
        "        for n in range(int((end_date - start_date).days)):\n",
        "            yield start_date + timedelta(n)\n",
        "    \n",
        "    base_url = \"https://www.zeit.de/gsitemaps/index.xml?date=\"\n",
        "    # We have to trick zeit.de into thinking we are running the requests\n",
        "    # using the library requests:\n",
        "    headers = {\"User-Agent\": \"python-requests/2.21.0\"}\n",
        "\n",
        "    # Start client session:\n",
        "    async with aiohttp.ClientSession(\n",
        "          connector=aiohttp.TCPConnector(limit=5),\n",
        "          cookie_jar=aiohttp.CookieJar(),\n",
        "          headers=headers\n",
        "        ) as session:\n",
        "        # Run a request to set a cookie for the session:\n",
        "        await session.request(method=\"GET\", url=\"https://www.zeit.de/gsitemaps/index.xml\")\n",
        "        \n",
        "        async with aiosqlite.connect(article_db) as db:\n",
        "          tasks = []\n",
        "          for single_date in daterange(start_date, end_date):\n",
        "              url = base_url + single_date.strftime(\"%Y-%m-%d\")\n",
        "              tasks.append(\n",
        "                  write_urls_to_db(db_session=db, url=url, session=session)\n",
        "              )\n",
        "\n",
        "          responses = []\n",
        "          for f in tqdm.tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
        "              responses.append(await f)\n",
        "\n",
        "          await db.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ2JgvtiLH7-",
        "colab_type": "text"
      },
      "source": [
        "Retrieve all URLs listed in their sitemap for a defined period:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXXlNB9LvvFj",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "start_date = date(year=2020, month=1, day=1) #including\n",
        "end_date = date(year=2020, month=2, day=1) #excluding\n",
        "\n",
        "start_time = time.time()\n",
        "loop = asyncio.get_event_loop()\n",
        "result = loop.run_until_complete(bulk_crawl_and_write(\n",
        "    article_db=article_db,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date\n",
        "))\n",
        "duration = time.time() - start_time\n",
        "print(f\"\\nDownloaded sites in {duration:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-bgQpBc_9w",
        "colab_type": "text"
      },
      "source": [
        "Let's see how many URLs we have collected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcit7pa9c83Y",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor = db.cursor()\n",
        "print(len(list(cursor.execute(\"SELECT * FROM articles WHERE downloaded = 'false'\"))))\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzWwZKlwfNdg",
        "colab_type": "text"
      },
      "source": [
        "If we have a look at the https://www.zeit.de/robots.txt, it tells us we should not touch those URLs that cointain:\n",
        "* /zeit/\n",
        "* /templates/\n",
        "* /hp_channels/\n",
        "* /send/\n",
        "* /suche/\n",
        "* /comment-thread\n",
        "* /liveblog-backend\n",
        "\n",
        "Most of these URLs won't be listed in the sitemap but we never know, so let's explicitly remove them from our list of URLs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WJLvItUftO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor = db.cursor()\n",
        "\n",
        "# Delete articles in english:\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/zeit/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/templates/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/hp_channels/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/send/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/suche/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/comment-thread/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/liveblog-backend/%'\")\n",
        "\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RewZdjSTgmyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_content(tag):\n",
        "    content = \"\"\n",
        "    for x in tag.contents:\n",
        "        if isinstance(x, str):\n",
        "            content += \" \" + x\n",
        "        else:\n",
        "            content += extract_content(x)\n",
        "    return content\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove line breaks\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "    \n",
        "    # Remove double whitespace\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_article(raw_html):\n",
        "    # Initiate result dict:\n",
        "    result = {\n",
        "      \"text\": \"\",\n",
        "      \"title\": \"\",\n",
        "      \"tags\": \"\",\n",
        "      \"authors\": \"\",\n",
        "      \"publishing_date\": \"\",\n",
        "    }\n",
        "    \n",
        "    html = BeautifulSoup(raw_html, \"html.parser\")\n",
        "\n",
        "    # Get publishing date:\n",
        "    publishing_date_html = html.find(\"time\", class_=\"metadata__date\")\n",
        "    if publishing_date_html:\n",
        "        result[\"publishing_date\"] = publishing_date_html.get(\"datetime\", \"\")\n",
        "\n",
        "    # Get title:\n",
        "    title_html = (\n",
        "        html.find(\"span\", class_=\"article-heading__title\")\n",
        "        or html.find(\"span\", class_=\"column-heading__title\")\n",
        "        or html.find(\"span\", class_=\"headline__title\")\n",
        "        or html.find(\"span\", class_=\"article-header__title article-header__title--default\")\n",
        "    )\n",
        "    if title_html:\n",
        "        result[\"title\"] = title_html.text.strip()\n",
        "\n",
        "    # Get all tags:\n",
        "    tags_html = html.find_all(\"a\", class_=\"article-tags__link\")\n",
        "    if tags_html:\n",
        "        tags = [tag.text.replace(\" \", \"_\") for tag in tags_html]\n",
        "        result[\"tags\"] = \" \".join(tags)\n",
        "\n",
        "    # Get all authors:\n",
        "    authors_html = html.find(\"div\", class_=\"byline\")\n",
        "    if authors_html:\n",
        "        authors = [\n",
        "            author.text.replace(\" \", \"_\")\n",
        "            for author\n",
        "            in authors_html.find_all(\"span\", itemprop=\"name\")\n",
        "        ]\n",
        "        result[\"authors\"] = \" \".join(authors)\n",
        "\n",
        "    # Parse article text:\n",
        "    text = \"\"\n",
        "    paragraphs = html.find_all(\"p\", class_=\"paragraph article__item\")\n",
        "    if paragraphs:\n",
        "        for paragraph in paragraphs:\n",
        "            text += extract_content(paragraph)\n",
        "    result[\"text\"] = clean_text(text)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "async def fetch_html(url, session):\n",
        "    try:\n",
        "        response = await session.request(method=\"GET\", url=url)\n",
        "        response.raise_for_status()\n",
        "        return await response.text()        \n",
        "    except (\n",
        "        aiohttp.ClientError,\n",
        "        aiohttp.ClientResponseError,\n",
        "        aiohttp.http_exceptions.HttpProcessingError,\n",
        "    ) as e:\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "async def download_and_parse_article_url(url, article_id, session, db):\n",
        "    result = await fetch_html(url=url, session=session)\n",
        "    if result:\n",
        "        result_str = str(result)\n",
        "        if 'komplettansicht\" data-ct-label=\"all\"' in result_str:\n",
        "            result = await fetch_html(url=url + \"/komplettansicht\", session=session)\n",
        "        if result:\n",
        "            parsing_result = parse_article(result)\n",
        "            data = (\n",
        "                parsing_result[\"title\"],\n",
        "                parsing_result[\"text\"],\n",
        "                parsing_result[\"authors\"],\n",
        "                parsing_result[\"publishing_date\"],\n",
        "                parsing_result[\"tags\"],\n",
        "                \"true\",\n",
        "                article_id,\n",
        "            )\n",
        "            await db.execute(\"\"\"\n",
        "                UPDATE articles SET\n",
        "                    title = ?,\n",
        "                    text = ?,\n",
        "                    authors = ?,\n",
        "                    publishing_date = ?,\n",
        "                    tags = ?,\n",
        "                    downloaded = ?\n",
        "                WHERE article_id = ?\n",
        "                \"\"\",\n",
        "                data\n",
        "            )\n",
        "\n",
        "\n",
        "async def download_and_parse_text_of_all_article_urls(articles_db):\n",
        "    async with aiohttp.ClientSession(\n",
        "          connector=aiohttp.TCPConnector(limit=30),\n",
        "          cookie_jar=aiohttp.CookieJar(),\n",
        "          headers={\"User-Agent\": \"python-requests/2.21.0\"},\n",
        "          timeout=aiohttp.ClientTimeout(total=None),\n",
        "        ) as session:\n",
        "\n",
        "        # Run initial request to set a cookie:\n",
        "        await session.request(method=\"GET\", url=\"https://www.zeit.de\")\n",
        "\n",
        "        tasks = []\n",
        "        async with aiosqlite.connect(articles_db) as db:\n",
        "            db.row_factory = aiosqlite.Row\n",
        "            cursor = await db.execute(\"SELECT rowid, url FROM articles WHERE downloaded = 'false'\")\n",
        "            fetchall = await cursor.fetchall()\n",
        "            for row in fetchall:\n",
        "                tasks.append(\n",
        "                    download_and_parse_article_url(\n",
        "                        url=row[\"url\"],\n",
        "                        article_id=row[\"article_id\"],\n",
        "                        session=session,\n",
        "                        db=db\n",
        "                    )\n",
        "                )\n",
        "            for f in tqdm.tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
        "                await f\n",
        "            await db.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs7YWl0ycw98",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "loop = asyncio.get_event_loop()\n",
        "result = loop.run_until_complete(download_and_parse_text_of_all_article_urls(articles_db=article_db))\n",
        "duration = time.time() - start_time\n",
        "print(f\"\\nDownloaded sites in {duration:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wie7ir9Qd53L",
        "colab_type": "text"
      },
      "source": [
        "Depending on the time of the day and the capacity of the server, we might trigger some 503 (or 104) errors, meaning the server ran out of resources to fulfill our request. However, we can just rerun the cell above to redownload those that failed in the first run. There also might be some 404 errors indicating dead links. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY2ZATg2cffW",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor = db.cursor()\n",
        "num_downloaded = len(list(cursor.execute(\"SELECT * FROM articles WHERE downloaded = 'true'\")))\n",
        "num_not_downloaded = len(list(cursor.execute(\"SELECT * FROM articles WHERE downloaded = 'false'\")))\n",
        "db.close()\n",
        "\n",
        "print(f\"URLs downloaded: {num_downloaded}\")\n",
        "print(f\"URLs not downloaded: {num_not_downloaded}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPOBYWYFw7rp",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor = db.cursor()\n",
        "\n",
        "num_rows_before = len(list(cursor.execute(\"SELECT * FROM articles\")))\n",
        "\n",
        "# Let's delete everything that has not been downloaded at this point, (probaly only dead urls left)\n",
        "cursor.execute(\"DELETE FROM articles WHERE downloaded = 'false'\")\n",
        "\n",
        "# Delete articles without text:\n",
        "cursor.execute(\"DELETE FROM articles WHERE downloaded = 'true' AND text = '' \")\n",
        "\n",
        "# Delete articles in english:\n",
        "cursor.execute(\"DELETE FROM articles WHERE text LIKE '%Lesen Sie diesen Text auf Deutsch%'\")\n",
        "\n",
        "db.commit()\n",
        "\n",
        "num_rows_after = len(list(cursor.execute(\"SELECT * FROM articles\")))\n",
        "db.close()\n",
        "\n",
        "print(num_rows_before)\n",
        "print(num_rows_after)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QS9QyWuKhq6",
        "colab_type": "text"
      },
      "source": [
        "#3 Topic Modelling using Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5vj-VJZWLen",
        "colab_type": "text"
      },
      "source": [
        "##3.1 Preperations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SETQ09GSOPd",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "!python -m spacy download de_core_news_sm\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIKpINvRdG0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import de_core_news_sm\n",
        "nlp = de_core_news_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygKpqWeQLlH",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "# Let's download some extra stopwords:\n",
        "!wget https://raw.githubusercontent.com/solariz/german_stopwords/master/german_stopwords_full.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOXfZ4yaSn8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine stopwords from spacy and nltk and solariz:\n",
        "stopwords_solariz = set()\n",
        "with open('german_stopwords_full.txt') as f:\n",
        "    for word in f:\n",
        "        if not word.startswith(';'):\n",
        "            stopwords_solariz.add(word.strip())\n",
        "\n",
        "stopwords_spacy = spacy.lang.de.STOP_WORDS\n",
        "\n",
        "stopwords_nltk = nltk.corpus.stopwords.words('german')\n",
        "\n",
        "own_stopwords = set(\n",
        "    [\n",
        "     'hauptsache',\n",
        "     'jetzig',\n",
        "     'mittlerweile',\n",
        "     'freilich',\n",
        "     'zuvor',\n",
        "     'fortan',\n",
        "     'vorab',\n",
        "     'einzig',\n",
        "     'bloß',\n",
        "     'worauf'\n",
        "    ]\n",
        ")\n",
        "\n",
        "stopwords = stopwords_spacy | set(stopwords_nltk) | stopwords_solariz | own_stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwwrDSciWRHn",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Data preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdAsN0dmwGRL",
        "colab_type": "text"
      },
      "source": [
        "Data preprocessing steps:\n",
        "\n",
        "1. remove newline characters and multiple consecutive whitespaces\n",
        "2. remove quotation marks\n",
        "3. remove punctuation\n",
        "4. remove numerals\n",
        "5. lowercase\n",
        "6. tokenization\n",
        "7. remove stopwords\n",
        "8. Lemmatization\n",
        "9. bigram and trigram collocation detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF6m0ehPDbbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add new column to table:\n",
        "db = sqlite3.connect(str(article_db))\n",
        "db.execute(\"ALTER TABLE articles ADD preprocessed_text TEXT\")\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tRSHUSU4BG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text, stopwords):\n",
        "  tokens = gensim.utils.simple_preprocess(text, deacc=False) # takes care of 1.-6.\n",
        "  \n",
        "  # Remove stopwords\n",
        "  tokens = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "  # Lemmatization\n",
        "  allowed_postags = set(['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "  doc = nlp(\" \".join(tokens))\n",
        "  tokens = [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "RzHmpYOqDbbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "db.row_factory = sqlite3.Row\n",
        "cursor_iter = db.cursor()\n",
        "cursor_writer = db.cursor()\n",
        "num_rows = len(list(cursor_iter.execute(\"SELECT * FROM articles\")))\n",
        "for row in tqdm.tqdm(cursor_iter.execute(\"SELECT * FROM articles\"), total=num_rows):\n",
        "    preprocessed_text = preprocess_text(row[\"text\"], stopwords)\n",
        "    data = (\" \".join(preprocessed_text), row[\"article_id\"],)\n",
        "    cursor_writer.execute(\n",
        "        \"UPDATE articles SET preprocessed_text = ? WHERE article_id = ?\", data\n",
        "    )\n",
        "\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNVoY1aciHy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_over_preprocessed_documents(article_db):\n",
        "    db = sqlite3.connect(str(article_db))\n",
        "    db.row_factory = sqlite3.Row\n",
        "    cursor = db.cursor()\n",
        "    db.row_factory = sqlite3.Row\n",
        "    for row in cursor.execute(\"SELECT * FROM articles\"):\n",
        "      yield row[\"preprocessed_text\"].split(\" \")\n",
        "    db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC9BugufiEeF",
        "colab_type": "text"
      },
      "source": [
        "Bigram and Trigram collocation detection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRpWBvIW4Dsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram = gensim.models.Phrases(\n",
        "    iterate_over_preprocessed_documents(article_db),\n",
        "    min_count=10,\n",
        "    threshold=0.6,\n",
        "    scoring=\"npmi\"\n",
        ")\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "trigram = gensim.models.Phrases(\n",
        "    bigram_mod[iterate_over_preprocessed_documents(article_db)],\n",
        "    min_count=10,\n",
        "    threshold=0.8,\n",
        "    scoring=\"npmi\"\n",
        ")  \n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZL2gm52ML0n",
        "colab_type": "text"
      },
      "source": [
        "Mark bigrams and trigrams:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBC2wDUD4Fn9",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "db.row_factory = sqlite3.Row\n",
        "cursor_iter = db.cursor()\n",
        "cursor_writer = db.cursor()\n",
        "num_rows = len(list(cursor_iter.execute(\"SELECT * FROM articles\")))\n",
        "for row in tqdm.tqdm(cursor_iter.execute(\"SELECT * FROM articles\"), total=num_rows):\n",
        "    text = row['preprocessed_text'].split(\" \")\n",
        "    trigrammed_text = trigram_mod[bigram_mod[text]]\n",
        "    data = (\" \".join(trigrammed_text), row['article_id'], )\n",
        "    cursor_writer.execute(\n",
        "        \"UPDATE articles SET preprocessed_text = ? WHERE article_id = ?\",\n",
        "        data\n",
        "    )\n",
        "\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXYv644cJDuo",
        "colab_type": "text"
      },
      "source": [
        "Load some data from database into pandas table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fceSdBCeDbcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "df = pd.read_sql('SELECT title, preprocessed_text, publishing_date FROM articles', con=db)\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8AoPXsMDbcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop rows without title and publishing date:\n",
        "df = df.drop(df[(df['title'] == '') | (df['publishing_date'] == '')].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP3MgfLFDbcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['preprocessed_text'] = df['preprocessed_text'].str.split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsGJ4fIeDbcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['publishing_date'] = pd.to_datetime(df['publishing_date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO_t64C2DbcM",
        "colab_type": "text"
      },
      "source": [
        "Create Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EulFp1K54JW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(df['preprocessed_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxadRomkDbcO",
        "colab_type": "text"
      },
      "source": [
        "To improve the overall quality of the model, we can remove words that only occur a few times and words that occur in almost all documents. Let's look at those words first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "id": "udIEWUW4DbcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Words that only occur 10 times or less:\n",
        "for token_id, freq in dictionary.cfs.items():\n",
        "    if freq <= 10:\n",
        "        print(dictionary[token_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Qea7OpmfDbcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Words that occur in more than half of all documents:\n",
        "ratio = int(0.5 * df.shape[0])\n",
        "for token_id, n_docs in dictionary.dfs.items():\n",
        "    if n_docs >= ratio:\n",
        "        print(dictionary[token_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eucFPPcfDbcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove words that appear less than 10, and which appear in more 50% of all documents.\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIYMLVy5DbcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create corpus:\n",
        "corpus = [dictionary.doc2bow(text) for text in df['preprocessed_text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OITzi5Zv4LIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[0:1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyeIaODd2SDl",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "print(f\"Number of unique tokens: {len(dictionary)}\")\n",
        "print(f\"Number of documents: {df.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ZkIKHoyKDbcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "plt.hist([len(text) for text in corpus], bins=200)\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.xlabel('Number of Words (after preprocessing)')\n",
        "plt.show()\n",
        "plt.savefig(plots_dir / \"n_words_histrogram.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1t5A9kzWXsO",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Training the LDA model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5zOQ8XD4ONP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(filename='lda_model.log', level=logging.DEBUG)\n",
        "\n",
        "# Log the perplexity and coherence score at the end of each epoch:\n",
        "perplexity_logger = gensim.models.callbacks.PerplexityMetric(\n",
        "    corpus=corpus,\n",
        "    logger=\"shell\"\n",
        ")\n",
        "coherence_logger = gensim.models.callbacks.CoherenceMetric(\n",
        "    corpus=corpus,\n",
        "    texts=df['preprocessed_text'],\n",
        "    coherence=\"c_v\",\n",
        "    logger=\"shell\"\n",
        ")\n",
        "\n",
        "lda_model = gensim.models.ldamulticore.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=20, \n",
        "    random_state=100,\n",
        "    chunksize=1500,\n",
        "    passes=20,\n",
        "    alpha=\"auto\",\n",
        "    per_word_topics=True,\n",
        "    iterations=2000,\n",
        "    callbacks=[coherence_logger, perplexity_logger]\n",
        ")\n",
        "\n",
        "lda_model.save(str(models_dir / \"lda_model\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBfAE8DgDbcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parse logs for coherence score, plot over epochs:\n",
        "with open('lda_model.log', 'r') as f:\n",
        "    log = f.read()\n",
        "\n",
        "coherence_score = re.findall(\n",
        "    r'INFO:gensim\\.models\\.ldamodel:Epoch (\\d+): Coherence estimate: (\\d+\\.\\d+)',\n",
        "    log\n",
        ")\n",
        "\n",
        "perplexity_score = re.findall(\n",
        "    r'INFO:gensim\\.models\\.ldamodel:Epoch (\\d+): Perplexity estimate: (\\d+\\.\\d+)',\n",
        "    log\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03JHRD0xDbch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Coherence score vs epochs:\n",
        "ax = plt.figure().gca()\n",
        "plt.plot([int(x[0]) for x in coherence_score], [float(x[1]) for x in coherence_score])\n",
        "plt.grid(True)\n",
        "plt.ylabel('Coherence Score')\n",
        "plt.xlabel('Epoch')\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "plt.show()\n",
        "plt.savefig(plots_dir / \"coherence_score_vs_epochs.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBtlQVA6Dbcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.figure().gca()\n",
        "plt.plot([int(x[0]) for x in perplexity_score], [float(x[1]) for x in perplexity_score])\n",
        "plt.grid(True)\n",
        "plt.ylabel('Perplexity Score')\n",
        "plt.xlabel('Epoch')\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "plt.show()\n",
        "plt.savefig(plots_dir / \"perplexity_score_vs_epochs.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prn-KbPHM1m5",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, we could load a previously trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Akn8HjAM0nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model = gensim.models.ldamulticore.LdaModel.load(str(models_dir / \"lda_model\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo78wEjQ4Sbj",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "# Compute Perplexity\n",
        "print(f\"Perplexity: {lda_model.log_perplexity(corpus):.3f}\")\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = gensim.models.CoherenceModel(\n",
        "    model=lda_model,\n",
        "    texts=df['preprocessed_text'],\n",
        "    dictionary=dictionary,\n",
        "    coherence=\"c_v\"\n",
        ")\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_lda:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcK-etZpWher",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Training the LDA model with mallet:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euzSa9XRZV60",
        "colab_type": "text"
      },
      "source": [
        "Mallet is a different implementation that usese Gibbs Sampling which is a bit more accurate:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtrjUHJb4WAv",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip -P \"$EXTRA_LIBRARIES\"\n",
        "!unzip -a \"$EXTRA_LIBRARIES/mallet-2.0.8.zip\" -d \"$EXTRA_LIBRARIES/mallet-2.0.8\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCZkZwnu4Ylo",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "mallet_path = extra_libraries_dir / \"mallet-2.0.8\" / \"bin\" / \"mallet\"\n",
        "ldamallet = gensim.models.wrappers.LdaMallet(\n",
        "    mallet_path=str(mallet_path),\n",
        "    corpus=corpus,\n",
        "    num_topics=20,\n",
        "    id2word=dictionary,\n",
        "    iterations=2000,\n",
        ")\n",
        "# Transform mallet model to gensim compatible model: \n",
        "# (requires gensim > 3.7.0, before it will work, but has some bugs)\n",
        "lda_mallet_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
        "lda_mallet_model.save(str(models_dir / \"lda_mallet_model\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbC6wFM4L3o7",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, we could load a previously trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZa8irTVL-jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_mallet_model = gensim.models.LdaModel.load(str(models_dir / \"lda_mallet_model\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUnQhrUvGdwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute Coherence Score\n",
        "coherence_model_lda_mallet = gensim.models.CoherenceModel(\n",
        "    model=lda_mallet_model,\n",
        "    texts=df['preprocessed_text'],\n",
        "    dictionary=dictionary,\n",
        "    coherence=\"c_v\"\n",
        ")\n",
        "coherence_lda_mallet = coherence_model_lda_mallet.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_lda_mallet:.3f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8uz0o4sDbdA",
        "colab_type": "text"
      },
      "source": [
        "## 3.5 Compare both models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O5VTAJuDbcz",
        "colab_type": "text"
      },
      "source": [
        "Display topics and keyowrds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pldpa3HCIkQa",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "# LDA model (trained with Gensim)\n",
        "pprint(lda_model.show_topics(20, 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDI9HIcXGHw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lda mallet model (trained with mallet via Gensim wrapper)\n",
        "pprint(lda_mallet_model.show_topics(20, 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQVX-jiLDbc9",
        "colab_type": "text"
      },
      "source": [
        "Find most representative article for each topic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "dwHGuVI9Dbc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_most_representative_article_for_each_topic(model):\n",
        "    most_dominant_topic = defaultdict(lambda: {'topic_percentage': 0})\n",
        "\n",
        "    for document_index, topic_results in enumerate(model[corpus]):\n",
        "        if len(topic_results) == 3: # to account for difference between gensim and mallet \n",
        "          topic_results = topic_results[0]\n",
        "        for topic_number, topic_weight in topic_results:\n",
        "            if topic_weight > most_dominant_topic[topic_number]['topic_percentage']:\n",
        "                most_dominant_topic[topic_number] = {\n",
        "                    'topic_percentage': topic_weight,\n",
        "                    'document_index': document_index\n",
        "                }\n",
        "\n",
        "    for topic_num, values in sorted(most_dominant_topic.items()):\n",
        "        print(f\"Topic Number {topic_num}: Most representative article {values['document_index']} with {values['topic_percentage']:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlpMlHHAGlSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LDA model (trained with Gensim)\n",
        "find_most_representative_article_for_each_topic(lda_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSAPcASNGp8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lda mallet model (trained with mallet via Gensim wrapper)\n",
        "find_most_representative_article_for_each_topic(lda_mallet_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SzES5kEJxcAt"
      },
      "source": [
        "#3 LDA Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRSHUrzhDbdE",
        "colab_type": "text"
      },
      "source": [
        "First, let's calculate visually distinct colors for each topic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWyQB3S6DbdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_topics = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "gncbQJgHDbdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/taketwo/glasbey \"$EXTRA_LIBRARIES/glasbey\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "-7qBIkfADbdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append(str(extra_libraries_dir / 'glasbey'))\n",
        "from glasbey import Glasbey\n",
        "gb = Glasbey()\n",
        "p = gb.generate_palette(size=num_topics + 1)\n",
        "rgb = gb.convert_palette_to_rgb(p)\n",
        "# Transform to hex colors:\n",
        "colors = ['#%02x%02x%02x' % color for color in rgb]\n",
        "# Exclude 1st color since it is white\n",
        "colors = colors[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r35HHfkrpnxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58B3hMixhWz",
        "colab_type": "text"
      },
      "source": [
        "##3.1 Word clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFwFDUyDmnmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_topics = 19"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWAyWSQJxbbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_word_clouds(model, num_topics, result_filepath):\n",
        "    cloud = WordCloud(\n",
        "        background_color=\"white\",\n",
        "        width=1000,\n",
        "        height=1000,\n",
        "        max_words=15,\n",
        "        colormap=\"tab10\",\n",
        "        color_func=lambda *args, **kwargs: colors[i],\n",
        "        prefer_horizontal=1.0\n",
        "    )\n",
        "\n",
        "    topics = model.show_topics(num_topics=num_topics, num_words=20, formatted=False)\n",
        "\n",
        "    fig, axes = plt.subplots(math.ceil(num_topics/4), 4, figsize=(20, math.ceil(num_topics/4)*5), sharex=True, sharey=True)\n",
        "\n",
        "    for i, ax in enumerate(axes.flatten()):\n",
        "        fig.add_subplot(ax)\n",
        "        if i < num_topics:\n",
        "            topic_words = dict(topics[i][1])\n",
        "            cloud.generate_from_frequencies(topic_words, max_font_size=150)\n",
        "            plt.gca().imshow(cloud)\n",
        "            plt.gca().set_title(f\"Topic {i}\", fontdict=dict(size=25))\n",
        "        plt.gca().axis(\"off\")\n",
        "\n",
        "    plt.tight_layout(h_pad=10, w_pad=10)\n",
        "    plt.axis(\"off\")\n",
        "    #plt.margins(x=0, y=0)\n",
        "\n",
        "    plt.savefig(result_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFEXVeKagrvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_word_clouds(\n",
        "    model=lda_mallet_model,\n",
        "    num_topics=num_topics,\n",
        "    result_filepath=plots_dir / 'word_cloud_lda_mallet.png'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmOtRiU1gzGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_word_clouds(\n",
        "    model=lda_model,\n",
        "    num_topics=num_topics,\n",
        "    result_filepath=plots_dir / 'word_cloud_lda.png'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCVo46YaySWc",
        "colab_type": "text"
      },
      "source": [
        "##3.2 pyLDAvis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heNq3vasl-ds",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "# Visualize the topics\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIOdBxcWDbdL",
        "colab_type": "text"
      },
      "source": [
        "LDA model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij8lJREFDbdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_lda = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(p_lda, 'pyLDAvis_lda.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ze68uxPDbdN",
        "colab_type": "text"
      },
      "source": [
        "LDA mallet model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gNAiQgDbdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_mallet = pyLDAvis.gensim.prepare(lda_mallet_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(p_mallet, 'pyLDAvis_lda_mallet.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ1ijMQzkEZ0",
        "colab_type": "text"
      },
      "source": [
        "##3.3 t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsSit-Omr0er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = lda_model\n",
        "#model = lda_mallet_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "d5MyuSYPDbdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "array = []\n",
        "for topic_results in model[corpus]:  \n",
        "    topic_weights = [0]*num_topics\n",
        "    if len(topic_results) == 3:\n",
        "        topic_results = topic_results[0]\n",
        "    for topic_n, topic_weigth in topic_results:\n",
        "        topic_weights[topic_n] = topic_weigth\n",
        "    array.append(topic_weights)\n",
        "\n",
        "topic_weights_array = np.array(array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW9SkwdLkKCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(\n",
        "    n_components=2,\n",
        "    random_state=0,\n",
        "    perplexity=20,\n",
        "    early_exaggeration=120,\n",
        ")\n",
        "embedding = tsne.fit_transform(topic_weights_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzN4sAeNqpyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"x\"], df[\"y\"] = embedding[:,0], embedding[:,1]\n",
        "df['dominant_topic'] = topic_weights_array.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqL8Is20lFc3",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\n",
        "from bokeh.models.widgets import DateRangeSlider\n",
        "from bokeh.layouts import column\n",
        "from bokeh.palettes import all_palettes\n",
        "output_notebook()\n",
        "\n",
        "source = ColumnDataSource(\n",
        "        data=dict(\n",
        "            x = df.x,\n",
        "            y = df.y,\n",
        "            colors = [colors[i] for i in df['dominant_topic']],\n",
        "            title = df.title,\n",
        "            day = df['publishing_date'],\n",
        "            day_humanreadable = [x.strftime(\"%d.%m.%Y\") for x in df.publishing_date],\n",
        "            alpha = [0.9] * df.shape[0],\n",
        "            size = [7] * df.shape[0]\n",
        "        )\n",
        "    )\n",
        "\n",
        "hover_tsne = HoverTool(names=[\"df\"], tooltips=\"\"\"\n",
        "    <div style=\"margin: 10\">\n",
        "        <div style=\"margin: 0 auto; width:300px;\">\n",
        "            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n",
        "            <span style=\"font-size: 12px\">@title (@day_humanreadable)</span>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
        "plot_tsne = figure(\n",
        "    plot_width=1000,\n",
        "    plot_height=700,\n",
        "    tools=tools_tsne,\n",
        "    title='t-SNE Visualization'\n",
        ")\n",
        "plot_tsne.xaxis.major_label_text_color = None\n",
        "plot_tsne.yaxis.major_label_text_color = None\n",
        "\n",
        "plot_tsne.circle(\n",
        "    x='x',\n",
        "    y='y',\n",
        "    size='size',\n",
        "    fill_color='colors', \n",
        "    alpha='alpha',\n",
        "    line_alpha=0,\n",
        "    line_width=0.01,\n",
        "    source=source,\n",
        "    name=\"df\"\n",
        ")\n",
        "\n",
        "callback = CustomJS(\n",
        "    args=dict(source=source),\n",
        "    code=\"\"\"\n",
        "        var data = source.data;\n",
        "        var start_date = cb_obj.value[0]\n",
        "        var end_date = cb_obj.value[1]\n",
        "        x = data['x']\n",
        "        y = data['y']\n",
        "        colors = data['colors']\n",
        "        alpha = data['alpha']\n",
        "        title = data['title']\n",
        "        day = data['day']\n",
        "        size = data['size']\n",
        "        for (i = 0; i < x.length; i++) {\n",
        "            if (day[i] <= end_date && day[i] >= start_date) {\n",
        "                alpha[i] = 0.9\n",
        "                size[i] = 4\n",
        "            } else {\n",
        "                alpha[i] = 0.01\n",
        "                size[i] = 2\n",
        "            }\n",
        "        }\n",
        "        source.change.emit();\n",
        "        \"\"\"\n",
        ")\n",
        "date_range_slider = DateRangeSlider(\n",
        "    title=\"Date Range: \",\n",
        "    start=min(df['publishing_date']),                     \n",
        "    end=max(df['publishing_date']),\n",
        "    value=(min(df['publishing_date']), max(df['publishing_date'])),\n",
        "    step=1,\n",
        "    width=980\n",
        ")\n",
        "date_range_slider.js_on_change('value', callback)\n",
        "\n",
        "layout = column(date_range_slider, plot_tsne)\n",
        "show(layout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgUpAnFFDbdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bokeh.plotting import output_file, save\n",
        "output_file(plots_dir / \"tsne-lda_mallet.html\")\n",
        "save(layout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTgeTvN-DbdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for Tensorflow's Embedding Projector:\n",
        "\n",
        "# Tensor\n",
        "with open(plots_dir / 'lda_mallet_tensor.tsv', 'w') as f:\n",
        "    for document_topics in topic_weights_array:\n",
        "        for topic_weight in document_topics:\n",
        "            f.write(f\"{topic_weight}\\t\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# Metadata\n",
        "with open(plots_dir / 'lda_mallet_metadata_title.tsv','w') as f:\n",
        "    for _, row in df.iterrows():\n",
        "        f.write(f\"{row['title'].strip()} ({row['publishing_date'].strftime('%d.%m.%Y')})\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}