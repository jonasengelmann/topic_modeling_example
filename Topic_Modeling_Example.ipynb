{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Topic Modelling II.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExCjpiIoKacl",
        "colab_type": "text"
      },
      "source": [
        "#1 Building the text corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU8fyxvqp-qy",
        "colab_type": "text"
      },
      "source": [
        "To build a large text corpus, we will scrape some newspaper articles from zeit.de which are publicly available and easily to parse. Newspaper articles in general are a good start to get familiar with topic modelling since we can more or less anticipate distinct topics. \n",
        "\n",
        "To scrape, we will first navigate through zeit.de's sitemap and download all available urls to online articles for a given period. Then, we will download them, parse their content and save the resulting text in a text file, one article per line. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCvh7K6rtLRr",
        "colab_type": "text"
      },
      "source": [
        "We will be using Python's asyncio library which will allows us to write asynchronous code. This way, Python can request multiple articles from the server at once without having to wait for the first request to be completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUTPgdT-x587",
        "colab_type": "text"
      },
      "source": [
        "Install third party libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_P77IK4tKtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install aiohttp==3.6.2\n",
        "!pip install nest_asyncio==1.4.0\n",
        "!pip install aiosqlite==0.15.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9W87o5ox3U4",
        "colab_type": "text"
      },
      "source": [
        "Prepare folder structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y75aALetLnsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "article_dir = Path('articles')\n",
        "article_dir.mkdir(exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hsWrkkMTOn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sqllite database\n",
        "article_db = article_dir / 'articles.db'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKnmphkbvSjk",
        "colab_type": "text"
      },
      "source": [
        "We will need this, so asyncio works within Ipython notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YboV3sMcaj30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ58phyXBzX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "import aiosqlite\n",
        "import sqlite3\n",
        "import time\n",
        "import tqdm\n",
        "import traceback\n",
        "from datetime import date, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W0W1CMFUD6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "db.execute(\"\"\"\n",
        "  CREATE TABLE articles\n",
        "    (\n",
        "      article_id INTEGER PRIMARY KEY,\n",
        "      url TEXT UNIQUE,\n",
        "      title TEXT,\n",
        "      text TEXT,\n",
        "      authors TEXT,\n",
        "      publishing_date TEXT,\n",
        "      topic_ref TEXT,\n",
        "      tags TEXT,\n",
        "      downloaded TEXT\n",
        "    )\n",
        "\"\"\")\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-a11zwURFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "async def fetch_html(url, session):\n",
        "      response = await session.request(method=\"GET\", url=url)\n",
        "      response.raise_for_status()\n",
        "      content = await response.text()\n",
        "      return content\n",
        "\n",
        "async def parse_html(url, session):\n",
        "    try:\n",
        "        html = await fetch_html(url=url, session=session)\n",
        "    except (\n",
        "        aiohttp.ClientError,\n",
        "        aiohttp.http_exceptions.HttpProcessingError,\n",
        "    ) as e:\n",
        "        print(e)\n",
        "        return None\n",
        "    else:\n",
        "        soup = BeautifulSoup(html, features='lxml')\n",
        "        urls = [entry.loc.text for entry in soup.findAll('url')]\n",
        "        return urls\n",
        "\n",
        "async def write_urls_to_db(db_session, url, session):\n",
        "    result = await parse_html(url=url, session=session)\n",
        "    if result:\n",
        "        for article_url in result:\n",
        "            entry = (f'{article_url}', 'false',)\n",
        "            try:\n",
        "                await db_session.execute(\n",
        "                  \"INSERT OR IGNORE INTO articles (url, downloaded) VALUES (?,?)\",\n",
        "                  entry\n",
        "                )\n",
        "            except sqlite3.InterfaceError as e:\n",
        "                print(e)\n",
        " \n",
        "async def bulk_crawl_and_write(article_db, start_date, end_date):\n",
        "    def daterange(start_date, end_date):\n",
        "        '''Helper function to easily iterate over date range'''\n",
        "        for n in range(int((end_date - start_date).days)):\n",
        "            yield start_date + timedelta(n)\n",
        "    \n",
        "    base_url = \"https://www.zeit.de/gsitemaps/index.xml?date=\"\n",
        "    # We have to trick zeit.de into thinking we are running the requests\n",
        "    # using the library requests:\n",
        "    headers = {'User-Agent': 'python-requests/2.21.0'}\n",
        "    con = aiohttp.TCPConnector(limit=5)\n",
        "\n",
        "    # Start client session:\n",
        "    async with aiohttp.ClientSession(\n",
        "          connector=con,\n",
        "          cookie_jar=aiohttp.CookieJar(),\n",
        "          headers=headers\n",
        "        ) as session:\n",
        "        # Run a request to set a cookie for the session:\n",
        "        await session.request(method=\"GET\", url='https://www.zeit.de/gsitemaps/index.xml')\n",
        "        \n",
        "        async with aiosqlite.connect(article_db) as db:\n",
        "          tasks = []\n",
        "          for single_date in daterange(start_date, end_date):\n",
        "              url = base_url + single_date.strftime(\"%Y-%m-%d\")\n",
        "              tasks.append(\n",
        "                  write_urls_to_db(db_session=db, url=url, session=session)\n",
        "              )\n",
        "          #await asyncio.gather(*tasks)\n",
        "\n",
        "          responses = []\n",
        "          for f in tqdm.tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
        "              responses.append(await f)\n",
        "\n",
        "          await db.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ2JgvtiLH7-",
        "colab_type": "text"
      },
      "source": [
        "Retrieve all URLs listed in their sitemap for a defined period:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXXlNB9LvvFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_date = date(year=2020, month=1, day=1) #including\n",
        "end_date = date(year=2020, month=2, day=1) #excluding\n",
        "\n",
        "start_time = time.time()\n",
        "loop = asyncio.get_event_loop()\n",
        "result = loop.run_until_complete(bulk_crawl_and_write(\n",
        "    article_db=article_db,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date\n",
        "))\n",
        "duration = time.time() - start_time\n",
        "print(f\"\\nDownloaded sites in {duration} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-bgQpBc_9w",
        "colab_type": "text"
      },
      "source": [
        "Let's see how many URLs we have collected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcit7pa9c83Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor = db.cursor()\n",
        "print(len(list(cursor.execute(\"SELECT * FROM articles WHERE downloaded = 'false'\"))))\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzWwZKlwfNdg",
        "colab_type": "text"
      },
      "source": [
        "If we have a look at the https://www.zeit.de/robots.txt, it tells us we should not touch those URLs that cointain:\n",
        "* /zeit/\n",
        "* /templates/\n",
        "* /hp_channels/\n",
        "* /send/\n",
        "* /suche/\n",
        "* /comment-thread\n",
        "* /liveblog-backend\n",
        "\n",
        "Most of these URLs won't be listed in the sitemap but we never know, so let's explicitly remove them from our list of URLs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WJLvItUftO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor = db.cursor()\n",
        "\n",
        "# Delete articles in english:\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/zeit/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/templates/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/hp_channels/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/send/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/suche/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/comment-thread/%'\")\n",
        "cursor.execute(\"DELETE FROM articles WHERE url LIKE '%/liveblog-backend/%'\")\n",
        "\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RewZdjSTgmyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_content(tag):\n",
        "    content = ''\n",
        "    for x in tag.contents:\n",
        "        if isinstance(x, str):\n",
        "            content += ' ' + x\n",
        "        else:\n",
        "            content += extract_content(x)\n",
        "    return content\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove line breaks\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    \n",
        "    # Remove double whitespace\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_article(raw_html):\n",
        "    \n",
        "    result = {\n",
        "      'text': '',\n",
        "      'title': '',\n",
        "      'tags': '',\n",
        "      'authors': '',\n",
        "      'publishing_date': '',\n",
        "    }\n",
        "    \n",
        "    html = BeautifulSoup(raw_html, 'html.parser')\n",
        "\n",
        "    publishing_date_html = html.find('time', class_='metadata__date')\n",
        "    if publishing_date_html:\n",
        "      result['publishing_date'] = publishing_date_html.get('datetime', '')\n",
        "\n",
        "    title_html = html.find('span', class_='article-heading__title')\n",
        "    if title_html:\n",
        "      result['title'] = title_html.text\n",
        "\n",
        "    tags_html = html.find_all('a', class_='article-tags__link')\n",
        "    if tags_html:\n",
        "        tags = [tag.text.replace(' ', '_') for tag in tags_html]\n",
        "        result['tags'] = ' '.join(tags)\n",
        "\n",
        "    authors_html = html.find('div', class_='byline')\n",
        "    if authors_html:\n",
        "        authors = [\n",
        "            author.text.replace(' ', '_')\n",
        "            for author\n",
        "            in authors_html.find_all('span', itemprop=\"name\")\n",
        "        ]\n",
        "        result['authors'] = ' '.join(authors)\n",
        "\n",
        "    text = ''\n",
        "    paragraphs = html.find_all('p', class_='paragraph article__item')\n",
        "    if paragraphs:\n",
        "        for paragraph in paragraphs:\n",
        "            text += extract_content(paragraph)\n",
        "    result['text'] = clean_text(text)\n",
        "\n",
        "    return result\n",
        "\n",
        "async def fetch_html(url, session):\n",
        "    try:\n",
        "        response = await session.request(method=\"GET\", url=url)\n",
        "        response.raise_for_status()\n",
        "        return await response.text()        \n",
        "    except (\n",
        "        aiohttp.ClientError,\n",
        "        aiohttp.ClientResponseError,\n",
        "        aiohttp.http_exceptions.HttpProcessingError,\n",
        "    ) as e:\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "async def download_and_parse_article_url(url, article_id, session, db):\n",
        "    result = await fetch_html(url=url, session=session)\n",
        "    if result:\n",
        "        result_str = str(result)\n",
        "        if 'komplettansicht\" data-ct-label=\"all\"' in result_str:\n",
        "            result = await fetch_html(url=url + '/komplettansicht', session=session)\n",
        "        if result:\n",
        "            parsing_result = parse_article(result)\n",
        "            data = (\n",
        "                parsing_result['title'],\n",
        "                parsing_result['text'],\n",
        "                parsing_result['authors'],\n",
        "                parsing_result['publishing_date'],\n",
        "                parsing_result['tags'],\n",
        "                'true',\n",
        "                article_id,\n",
        "                )\n",
        "            await db.execute(\"\"\"\n",
        "                UPDATE articles SET\n",
        "                  title = ?,\n",
        "                  text = ?,\n",
        "                  authors = ?,\n",
        "                  publishing_date = ?,\n",
        "                  tags = ?,\n",
        "                  downloaded = ?\n",
        "                  WHERE article_id = ?\n",
        "                \"\"\", data\n",
        "              )\n",
        "\n",
        "async def download_and_parse_text_of_all_article_urls(articles_db):\n",
        "    con = aiohttp.TCPConnector(limit=30)\n",
        "    timeout = aiohttp.ClientTimeout(total=None)\n",
        "    headers = {'User-Agent': 'python-requests/2.21.0'}\n",
        "    async with aiohttp.ClientSession(\n",
        "          connector=con,\n",
        "          cookie_jar=aiohttp.CookieJar(),\n",
        "          headers=headers,\n",
        "          timeout=timeout\n",
        "        ) as session:\n",
        "        # Run initial request to set a cookie:\n",
        "        await session.request(method=\"GET\", url='https://www.zeit.de')\n",
        "\n",
        "        tasks = []\n",
        "        async with aiosqlite.connect(articles_db) as db:\n",
        "            cursor = await db.execute(\"SELECT rowid, url FROM articles WHERE downloaded = 'false'\")\n",
        "            fetchall = await cursor.fetchall()\n",
        "            for row in fetchall:\n",
        "                article_id = row[0]\n",
        "                url = row[1]\n",
        "                tasks.append(\n",
        "                    download_and_parse_article_url(url=url, article_id=article_id, session=session, db=db)\n",
        "                )\n",
        "            for f in tqdm.tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
        "              await f\n",
        "            await db.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs7YWl0ycw98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "loop = asyncio.get_event_loop()\n",
        "result = loop.run_until_complete(download_and_parse_text_of_all_article_urls(articles_db=article_db))\n",
        "duration = time.time() - start_time\n",
        "print(f\"\\nDownloaded sites in {duration} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wie7ir9Qd53L",
        "colab_type": "text"
      },
      "source": [
        "Depending on the time of the day and the capacity of the server, we might trigger some 503 errors, meaning the server ran out of resources to fulfill our request. However, we can just rerun the cell above to redownload those that failed in the first run. There also might be an occaional 404 error indicating a dead link. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY2ZATg2cffW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor = db.cursor()\n",
        "num_downloaded = len(list(cursor.execute(\"SELECT * FROM articles WHERE downloaded = 'true'\")))\n",
        "num_not_downloaded = len(list(cursor.execute(\"SELECT * FROM articles WHERE downloaded = 'false'\")))\n",
        "db.close()\n",
        "\n",
        "print(f'URLs downloaded: {num_downloaded}')\n",
        "print(f'URLs not downloaded: {num_not_downloaded}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPOBYWYFw7rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor = db.cursor()\n",
        "\n",
        "num_rows_before = len(list(cursor.execute(\"SELECT * FROM articles\")))\n",
        "\n",
        "# Let's delete everything that has not been downloaded at this point, (probaly only dead urls left)\n",
        "cursor.execute(\"DELETE FROM articles WHERE downloaded = 'false'\")\n",
        "\n",
        "# Delete articles without text:\n",
        "cursor.execute(\"DELETE FROM articles WHERE downloaded = 'true' AND text = '' \")\n",
        "\n",
        "# Delete articles in english:\n",
        "cursor.execute(\"DELETE FROM articles WHERE text LIKE '%Lesen Sie diesen Text auf Deutsch%'\")\n",
        "\n",
        "db.commit()\n",
        "\n",
        "num_rows_after = len(list(cursor.execute(\"SELECT * FROM articles\")))\n",
        "db.close()\n",
        "\n",
        "print(num_rows_before)\n",
        "print(num_rows_after)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QS9QyWuKhq6",
        "colab_type": "text"
      },
      "source": [
        "#2 Topic Modelling using Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5vj-VJZWLen",
        "colab_type": "text"
      },
      "source": [
        "##2.1 Preperations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ebRSw9NLyEK",
        "colab_type": "text"
      },
      "source": [
        "Note: in Colab the third-party packages are already installed, if you run this notebook locally, you might have to install them before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2mvfgIEdGx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import spacy\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SETQ09GSOPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download de_core_news_sm\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIKpINvRdG0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import de_core_news_sm\n",
        "nlp = de_core_news_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygKpqWeQLlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/solariz/german_stopwords/master/german_stopwords_full.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOXfZ4yaSn8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine stopwords from spacy and nltk and solariz:\n",
        "stopwords_solariz = set()\n",
        "with open('german_stopwords_full.txt') as f:\n",
        "    for word in f:\n",
        "        if not word.startswith(';'):\n",
        "            stopwords_solariz.add(word.strip())\n",
        "\n",
        "stopwords_spacy = spacy.lang.de.STOP_WORDS\n",
        "\n",
        "stopwords_nltk = nltk.corpus.stopwords.words('german')\n",
        "\n",
        "stopwords = stopwords_spacy | set(stopwords_nltk) | stopwords_solariz | set(['hauptsache', 'jetzig', 'mittlerweile', 'freilich', 'fortan'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwwrDSciWRHn",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Data preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdAsN0dmwGRL",
        "colab_type": "text"
      },
      "source": [
        "Data preprocessing steps:\n",
        "\n",
        "1. remove newline characters and multiple consecutive whitespaces\n",
        "2. remove quotation marks\n",
        "3. remove punctuation\n",
        "4. remove numerals\n",
        "5. lowercase\n",
        "6. tokenization\n",
        "7. remove stopwords\n",
        "8. Lemmatization\n",
        "9. bigram and trigram collocation detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVVWW2A0vr1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create new table for preprocessed text\n",
        "db = sqlite3.connect(str(article_db))\n",
        "db.execute('CREATE TABLE articles_preprocessed (article_id INTEGER UNIQUE, preprocessed_text TEXT)')\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tRSHUSU4BG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(text, stopwords):\n",
        "  # Takes care of 1. - 5.\n",
        "  tokens = gensim.utils.simple_preprocess(text, deacc=False) # takes care of 1.-5.\n",
        "  \n",
        "  # Remove stopwords\n",
        "  tokens = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "  # Lemmatization\n",
        "  allowed_postags= set(['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "  doc = nlp(\" \".join(tokens))\n",
        "  tokens = [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx8Qm8fo1vsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor_iter = db.cursor()\n",
        "cursor_writer = db.cursor()\n",
        "num_rows = len(list(cursor_iter.execute(\"SELECT * FROM articles\")))\n",
        "for row in tqdm.tqdm(cursor_iter.execute(\"SELECT * FROM articles\"), total=num_rows):\n",
        "    entry = (row[0], ' '.join(preprocessing(row[3], stopwords)),)\n",
        "    cursor_writer.execute(\"INSERT OR IGNORE INTO articles_preprocessed (article_id, preprocessed_text) VALUES (?,?)\", entry)\n",
        "\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNVoY1aciHy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_over_processed_documents(article_db):\n",
        "    db = sqlite3.connect(str(article_db))\n",
        "    cursor = db.cursor()\n",
        "    for row in cursor.execute(\"SELECT * FROM articles_preprocessed\"):\n",
        "      yield row[1].split(' ')\n",
        "    db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC9BugufiEeF",
        "colab_type": "text"
      },
      "source": [
        "Bigram and Trigram collocation detection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRpWBvIW4Dsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram = gensim.models.Phrases(\n",
        "    iterate_over_processed_documents(article_db),\n",
        "    min_count=10,\n",
        "    threshold=0.6,\n",
        "    scoring='npmi'\n",
        ")\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "trigram = gensim.models.Phrases(\n",
        "    bigram_mod[iterate_over_processed_documents(article_db)],\n",
        "    min_count=10,\n",
        "    threshold=0.8,\n",
        "    scoring='npmi'\n",
        ")  \n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZL2gm52ML0n",
        "colab_type": "text"
      },
      "source": [
        "Mark bigrams and trigrams:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBC2wDUD4Fn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(str(article_db))\n",
        "cursor_iter = db.cursor()\n",
        "cursor_writer = db.cursor()\n",
        "num_rows = len(list(cursor_iter.execute(\"SELECT * FROM articles_preprocessed\")))\n",
        "for row in tqdm.tqdm(cursor_iter.execute(\"SELECT * FROM articles_preprocessed\"), total=num_rows):\n",
        "    text = row[1].split(' ')\n",
        "    trigrammed_text = trigram_mod[bigram_mod[text]]\n",
        "    data = (' '.join(trigrammed_text), row[0], )\n",
        "    cursor_writer.execute(\"UPDATE articles_preprocessed SET preprocessed_text = ? WHERE article_id = ?\", data)\n",
        "\n",
        "db.commit()\n",
        "db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EulFp1K54JW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dictionary\n",
        "dictionary = gensim.corpora.Dictionary(iterate_over_processed_documents(article_db))\n",
        "\n",
        "# Remove words that appear less than 10, and which appear in more 60% of all documents.\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ9ZTidHirzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create bag of words corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in iterate_over_processed_documents(article_db)]\n",
        "\n",
        "documents = [x for x in iterate_over_processed_documents(article_db)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OITzi5Zv4LIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyeIaODd2SDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Number of unique tokens: {len(dictionary)}')\n",
        "print(f'Number of documents: {len(documents)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1t5A9kzWXsO",
        "colab_type": "text"
      },
      "source": [
        "##2.3 Training the LDA model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5zOQ8XD4ONP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.callbacks import PerplexityMetric\n",
        "from gensim.models.callbacks import CoherenceMetric\n",
        "\n",
        "\n",
        "# Log the perplexity and coherence score at the end of each epoch:\n",
        "perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')\n",
        "coherence_logger = CoherenceMetric(corpus=corpus, texts=documents, coherence=\"c_v\", logger=\"shell\")\n",
        "\n",
        "lda_model = gensim.models.ldamulticore.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=20, \n",
        "    random_state=100,\n",
        "    chunksize=1500,\n",
        "    passes=20,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True,\n",
        "    iterations=2000,\n",
        "    callbacks=[coherence_logger, perplexity_logger]\n",
        ")\n",
        "\n",
        "\n",
        "lda_model.save('lda_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prn-KbPHM1m5",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, you could load a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Akn8HjAM0nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model = LdaModel.load('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Wp0nWE4Qro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the most 30 distinctive keyword for all 20 topics\n",
        "pprint(lda_model.print_topics(20, 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo78wEjQ4Sbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute Perplexity\n",
        "print(f'Perplexity: {lda_model.log_perplexity(corpus)}')\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = gensim.models.CoherenceModel(\n",
        "    model=lda_model,\n",
        "    texts=documents,\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        ")\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print(f'Coherence Score: {coherence_lda}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcK-etZpWher",
        "colab_type": "text"
      },
      "source": [
        "##2.4 Training the LDA model with mallet:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euzSa9XRZV60",
        "colab_type": "text"
      },
      "source": [
        "Mallet is a different implementation that usese Gibbs Sampling which is a bit more accurate:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtrjUHJb4WAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip -a \"mallet-2.0.8.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCZkZwnu4Ylo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mallet_path = Path('mallet-2.0.8') / 'bin' / 'mallet'\n",
        "ldamallet = gensim.models.wrappers.LdaMallet(\n",
        "    mallet_path=str(mallet_path),\n",
        "    corpus=corpus,\n",
        "    num_topics=20,\n",
        "    id2word=dictionary,\n",
        "    iterations=2000,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pldpa3HCIkQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Topics\n",
        "pprint(ldamallet.show_topics(20, 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lV44QqO4ZPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute Coherence Score\n",
        "coherence_model_ldamallet = gensim.models.CoherenceModel(model=ldamallet, texts=documents, dictionary=dictionary, coherence='c_v')\n",
        "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
        "print(f'Coherence Score: {coherence_ldamallet}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzES5kEJxcAt",
        "colab_type": "text"
      },
      "source": [
        "#3 LDA Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58B3hMixhWz",
        "colab_type": "text"
      },
      "source": [
        "##3.1 Word clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWAyWSQJxbbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "cloud = WordCloud(stopwords=stopwords,\n",
        "                  background_color='white',\n",
        "                  width=1800,\n",
        "                  height=1000,\n",
        "                  max_words=15,\n",
        "                  colormap='tab10',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = ldamallet.show_topics(num_topics=10, num_words=20, formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(20,20), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=200)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=24))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "\n",
        "plt.savefig('word_clouds.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCVo46YaySWc",
        "colab_type": "text"
      },
      "source": [
        "##3.2 pyLDAvis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AKpqo0yNUf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyLDAvis==2.1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn75dpvNyUuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the topics\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}